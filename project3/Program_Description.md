## Andreas Gagas     Project 3    spellchecker.c - Network Spell Checker Program Description

## Design 
<br/>To begin, I started small, before establishing the network (given code), I defined a structure for all strings in the dictionary file (used DEFAULT DICTIONARY) to be stored. The function fill_dictionary_structure(), iterates through the file and copies each string (tokenize first space, null terminate the string) to char **dictionary_stored_here, who size was determined by count of words in the dictionary file. 
Next, the function check_dictionary(), implements a simple binary search algorithm. This method would continually halve the dictionary_stored_here structure and adjusts the “left” and “right” indexes, based on if the word is less than or greater than the middle term/word. The search would continue until the middle index matches the word, otherwise the word was misspelled, when left and right first overlap. 
<br/><br/>That functionality, is to be scaled … 
<br/><br/>The function create_threads() will spawn a thread pool of worker threads, who will “live” inside the their own worker_thread() function, , along with the single log thread, which will write to “data.log”. Also to be noted, init_some_vars() will initialize the two mutex locks, and 4 condition variables to be detailed later. 
<br/><br/>The addition of the given network code, the main thread, sits in a loop awaiting possible connections (accepts blocks until new socket), and is responsible for adding that socket file descriptor to a queue, which sets of a “chain reaction”. The socket/ connection queue is an example producer (main thread)-consumer (worker thread) issue that exists. In order for the main thread to add the socket, it needs to acquire the lock (only 1 thread can hold this), if the queue is full, release that lock and “wait” for the condition (safe_to_add) to be true. The condition will be true when the consumer thread removes a socket descriptor from the queue and signals to the waiting thread that its “safe_to_add”. The producer will re-aquire that lock, add the socket to the connection queue and signal to the other threads that its “safe_to_remove” and release that lock. Back and forth relationship between the two (synchronized), but the result is the two threads can’t access the connection queue at once. 
<br/><br/>The connection queue now contains a socket fd, and mentioned before, the consumer is signaled that its now safe to remove a socket fd. Each worker thread will sit in an infinite loop, with its first job being that removal of the socket fd. The server and client now have this path to directly communicate. A word is sent, and strok is used to cut off when the first space, tab, newline, or carriage return is encountered. That word is sent to check_dictionary() (design described at start), and “ OK” or “ MISSPELLED” is concatenated to the word. The concatenated string is communicated back to the client through simply write() to the socket fd. Client side is complete, and process is repeated. But for server-side, that concatenated string is added to another queue, the log queue. 
<br/><br/>The log thread, which also sits in an infinite loop, tackles the producer (worker thread) – consumer (log thread) in the exact fashion as the previous example. A separate lock, and two other condition variables (safe_to_add2 and safe_to_remove2) are implemented, in order to safely add and remove a string, followed by appending that string to the existing log file.  
<br/><br/>One of the last pieces I worked on, was initialization of the command line arguments, if provided. Spell Checker program takes 0,1, or 2 arguments and it doesn’t matter if the first argument is the port number or the dictionary file (vice-verse with the second, if provided). 
<br/><br/>Finally, I used valgrind to detect possible memory leaks, which were present as I didn’t free malloced memory/ heaps objects when spellchecker was killed (Process terminating with default action of signal 2 (SIGINT), which is ctrl-c). Catch the signal, and individually free all the strings in the double pointer structure, along with the structure used to store the word sent. Better practice is to solely use a flag when in the siglal handler, to allow the main loop to exit, then do cleanup, but there are a few blocking calls, like accept(), which would not catch the signal when it occured. 
The only issue is “possibly lost”, which is likely due to the threads not being in a detached or joined state, as they run in infinite loops. 
 but I sent cancel request to all worker threads when program is killed. 

## Testing 
Throughout development, I had printed an update to the server-side screen for each event. For example, transferring the dictionary file to be stored in a dictionary structure, I outputted to separate file and checked to see if they matched. I would input a word without a client and test the search algorithm. Tested the functionality of the queues, which work as just a fifo circular buffer. I set the buffer SIZE to 50, but regardless of the SIZE, when SIZE is reached, the index goes back to 0 to re-populate the "circle" again with integers ot strings. In addition, every system call was checked, to ensure the lock was properly initizliated, or the condition wait properly releases the lock before going on the "waiting on condition queue". 
<br/><br/>Also, during this time, shortly after the addition of the network code, I tested the server with the netcat utility/tool, using the loopback address (127.0.01) or local host, and use the same port number as my program. I would open multiple terminals, connect to same port, and test various words. The client would see the appropriate response, but the server’s “data.log” would not be complete, despite the server screen printing the appropriate concatenated string. To resolve, I would close the fptr and fflush the buffer/stream from the fptr for each run in the log thread loop. 
<br/><br/>After completion, I decided to do the same with telnet, and the response from the server was unintelligible, as was the data.log file. The issue was an additional carriage return (‘\r’) character was being added to the word, which set of a chain of junk being added. I added the carriage return character to the delimiters for strtok to tokenize, and the issue was resolved. 
<br/><br/>Testing the success of mutual exclusion and synchronization to prevent race conditions (ex. producer-consumer problem) is difficult to detect. Reading the textbook helped the most with quickly implementing locks and condition variables. After the implementation, I can't specifically cause a race condition. But, regardless of the terminals I connected and the various words (also best to test the same word over clients and see log file) I sent, I never got an invalid response (ex. duplicates), and the log file updated appropriately. 
<br/><br/>After receiving the test threads-possible python script, I did further testing, first only using only a single correct word in words.txt, and specifying the number of threads to create, and seeing the client side results with verbose.The resulting data.log file held the appropriate count and results, using grep -c option to count the instances of that word in data.log.
<br/><br/>Next, I decided to add mutiple correct words to the file, and saw erroneous behavior at least 60% of the time I would run the script. Looking at the detail statement I printed, my log queue was adding the strings correctly, but was doubling up on certain values when pulling a word out, which didn't exactly make sense. Despite the near textbook implementation to one possible solution to the consumer-producer problem, I thought I had a race condition. To expand, in instances where I would add an .000001 delay each word being sent from the script, there would be no issue. Previously I had a similar issue that I resolved by not only opening (for append) and closing the fptr, but also using using fflush on the buffer/stream. I decided to create the log file for writing before entering the main loop, and soley using fflush on the file pointer. As a result, the log queue and subsequent results were no longer acting strange (with the delay removed from the script).  
<br/><br/>I next tested the limits of the worker threads. For example, if I added 3 words to words.txt, set the worker thread count to 30, and ran the script to simulate 20 threads, I would get 20 instances of each word. If I set the worker thread count to 10 and ran the same command, I would only see 10 instances of word, which was expected as my program only had 10 concurrent worker threads to process 20 threads. 
 


